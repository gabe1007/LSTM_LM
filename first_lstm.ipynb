{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-04T01:06:15.874009Z",
     "iopub.status.busy": "2025-02-04T01:06:15.873730Z",
     "iopub.status.idle": "2025-02-04T01:06:15.878349Z",
     "shell.execute_reply": "2025-02-04T01:06:15.877464Z",
     "shell.execute_reply.started": "2025-02-04T01:06:15.873990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T01:05:06.279427Z",
     "iopub.status.busy": "2025-02-04T01:05:06.279171Z",
     "iopub.status.idle": "2025-02-04T01:05:06.447549Z",
     "shell.execute_reply": "2025-02-04T01:05:06.446939Z",
     "shell.execute_reply.started": "2025-02-04T01:05:06.279407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/tiny-shake/input.txt', \"r\", encoding=\"utf-8\") as my_file:\n",
    "    text = my_file.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T01:05:12.566117Z",
     "iopub.status.busy": "2025-02-04T01:05:12.565834Z",
     "iopub.status.idle": "2025-02-04T01:05:12.573511Z",
     "shell.execute_reply": "2025-02-04T01:05:12.572744Z",
     "shell.execute_reply.started": "2025-02-04T01:05:12.566094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 64\n",
    "    block_size = 128\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    learning_rate = 1e-3\n",
    "    grad_clip = 1.0\n",
    "    max_epochs = 20\n",
    "    weight_init_range = 0.1\n",
    "    num_batches_per_epoch = 100\n",
    "    num_eval_batches = 10\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "# Code to get the training and validation batches\n",
    "def get_batch(split, block_size=Config.block_size, batch_size=Config.batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch of data for training or testing.\n",
    "    \"\"\"\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for i in ix:\n",
    "        x_batch.append(data[i:i+block_size])\n",
    "        y_batch.append(data[i+1:i+1+block_size])\n",
    "\n",
    "    x = torch.stack(x_batch)\n",
    "    y = torch.stack(y_batch)\n",
    "    x, y = x.to(Config.device), y.to(Config.device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Generate text\n",
    "def generate(model, start_text, max_length, temperature):\n",
    "    model.eval()\n",
    "    chars = torch.tensor(encode(start_text)).unsqueeze(0).to(Config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(chars)\n",
    "            next_char_logits = logits[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_char_logits, dim=0)\n",
    "            next_char = torch.multinomial(probs, 1)\n",
    "            chars = torch.cat([chars, next_char.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return decode(chars[0].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy of the dataset\n",
    "\n",
    "Entropy, in the context of llms, refers to the amount of information a token carries. The bigger the entropy the harder it is for the model to predict the next word, or in our case, the next character.\n",
    "\n",
    "We are also measuring the bits pert byte, the number of bits a language model needs to represent one byte of the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T23:31:06.934914Z",
     "iopub.status.busy": "2025-02-02T23:31:06.934605Z",
     "iopub.status.idle": "2025-02-02T23:31:09.677965Z",
     "shell.execute_reply": "2025-02-02T23:31:09.677227Z",
     "shell.execute_reply.started": "2025-02-02T23:31:06.934889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram entropy: 4.7794 bits per character (0.5974 bits per byte)\n",
      "2-gram entropy: 4.1588 bits per character (0.5199 bits per byte)\n",
      "3-gram entropy: 3.6899 bits per character (0.4612 bits per byte)\n",
      "4-gram entropy: 3.3077 bits per character (0.4135 bits per byte)\n",
      "Conditional entropy with context size 1: 3.5383 bits per character (0.4423 bits per byte)\n",
      "Conditional entropy with context size 2: 2.7520 bits per character (0.3440 bits per byte)\n",
      "Conditional entropy with context size 3: 2.1610 bits per character (0.2701 bits per byte)\n",
      "Conditional entropy with context size 4: 1.7672 bits per character (0.2209 bits per byte)\n"
     ]
    }
   ],
   "source": [
    "def calculate_ngram_entropies(text, max_n = 4):\n",
    "    \"\"\"\n",
    "    Calculate entropy for different n-gram sizes up to max_n.\n",
    "    Returns both bits per character and bits per byte.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n in range(1, max_n + 1):\n",
    "        # Create overlapping n-grams\n",
    "        ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        \n",
    "        # Count frequencies\n",
    "        counts = Counter(ngrams)\n",
    "        total = len(ngrams)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        probs = [count/total for count in counts.values()]\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = -sum(p * log2(p) for p in probs)\n",
    "        \n",
    "        # Normalize by n to get per-character entropy\n",
    "        normalized_entropy = entropy / n\n",
    "        # Calculate bits per byte\n",
    "        bits_per_byte = normalized_entropy / 8\n",
    "        \n",
    "        results[n] = (normalized_entropy, bits_per_byte)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def calculate_conditional_entropy(text, context_size):\n",
    "    \"\"\"\n",
    "    Calculate conditional entropy H(X|Y) where Y is the context.\n",
    "    Returns both bits per character and bits per byte.\n",
    "    \"\"\"\n",
    "    # Get all possible contexts and their following characters\n",
    "    contexts = {}\n",
    "    for i in range(len(text) - context_size):\n",
    "        context = text[i:i+context_size]\n",
    "        next_char = text[i+context_size]\n",
    "        if context not in contexts:\n",
    "            contexts[context] = []\n",
    "        contexts[context].append(next_char)\n",
    "    \n",
    "    # Calculate conditional entropy\n",
    "    total_contexts = sum(len(chars) for chars in contexts.values())\n",
    "    entropy = 0\n",
    "    \n",
    "    for context, next_chars in contexts.items():\n",
    "        # Probability of this context\n",
    "        context_prob = len(next_chars) / total_contexts\n",
    "        \n",
    "        # Calculate entropy for characters following this context\n",
    "        char_counts = Counter(next_chars)\n",
    "        char_probs = [count/len(next_chars) for count in char_counts.values()]\n",
    "        context_entropy = -sum(p * log2(p) for p in char_probs)\n",
    "        \n",
    "        entropy += context_prob * context_entropy\n",
    "    \n",
    "    bits_per_byte = entropy / 8\n",
    "    return entropy, bits_per_byte\n",
    "\n",
    "# Calculate n-gram entropies\n",
    "ngram_entropies = calculate_ngram_entropies(text)\n",
    "for n, (bpc, bpb) in ngram_entropies.items():\n",
    "    print(f\"{n}-gram entropy: {bpc:.4f} bits per character ({bpb:.4f} bits per byte)\")\n",
    "\n",
    "# Calculate conditional entropies for different context sizes\n",
    "for context_size in range(1, 5):\n",
    "    bpc, bpb = calculate_conditional_entropy(text, context_size)\n",
    "    print(f\"Conditional entropy with context size {context_size}: \"\n",
    "          f\"{bpc:.4f} bits per character ({bpb:.4f} bits per byte)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the values of the consitional entropy, we can conclude that the words depend heavily on the previous context and not by chance. A language model is trained to minimize its cross entropy with respect to the training data. If the language model learns perfectly from its training data, the model’s cross entropy will be exactly the same as the entropy of the training data.We can think of a model’s cross entropy as its approximation of the entropy of its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T01:15:50.319354Z",
     "iopub.status.busy": "2025-02-04T01:15:50.319039Z",
     "iopub.status.idle": "2025-02-04T01:15:50.331726Z",
     "shell.execute_reply": "2025-02-04T01:15:50.331063Z",
     "shell.execute_reply.started": "2025-02-04T01:15:50.319326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShakeLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, Config.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(Config.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=Config.embedding_dim,\n",
    "            hidden_size=Config.hidden_dim,\n",
    "            num_layers=Config.num_layers,\n",
    "            dropout=Config.dropout,\n",
    "            batch_first=True # (batch_size, seq_length, input_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.dropout)\n",
    "        self.fc = nn.Linear(Config.hidden_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize embeddings and linear layer\n",
    "        self.embedding.weight.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "        self.fc.weight.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "        # Initialize LSTM weights and biases\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "            elif 'bias' in name:\n",
    "                # Initialize all biases to zero\n",
    "                param.data.zero_()\n",
    "                # Set forget gate bias to 1\n",
    "                if 'bias_ih' in name or 'bias_hh' in name:\n",
    "                    n = param.size(0)\n",
    "                    start, end = n//4, n//2\n",
    "                    param.data[start:end].fill_(1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # 64, 128 --> 64, 128, 256\n",
    "        normalized = self.layer_norm(embedded) \n",
    "        lstm_out, _ = self.lstm(normalized) #64, 128, 512\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        logits = self.fc(dropped) # 64, 128, 65\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    \n",
    "    for i in range(Config.num_batches_per_epoch):\n",
    "        x, y = get_batch('train')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(-1, C), y.view(-1))\n",
    "        perplexity = torch.exp(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=Config.grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_perplexity += perplexity.item()\n",
    "        \n",
    "    avg_loss = total_loss / Config.num_batches_per_epoch\n",
    "    # avg_perplexity = total_perplexity / Config.num_batches_per_epoch\n",
    "    bits_per_char = avg_loss / math.log(2)  # Convert nats to bits\n",
    "    bits_per_byte = bits_per_char / 8  # Convert bits per char to bits per byte\n",
    "    \n",
    "    return avg_loss, bits_per_byte\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    \n",
    "    for i in range(Config.num_eval_batches):\n",
    "        x, y = get_batch('val')\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(-1, C), y.view(-1))\n",
    "        perplexity = torch.exp(loss)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_perplexity += perplexity.item()\n",
    "        \n",
    "    avg_loss = total_loss / Config.num_eval_batches\n",
    "    # avg_perplexity = total_perplexity / Config.num_eval_batches\n",
    "    bits_per_char = avg_loss / math.log(2)  # Convert nats to bits\n",
    "    bits_per_byte = bits_per_char / 8  # Convert bits per char to bits per byte\n",
    "    \n",
    "    return avg_loss, bits_per_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T01:19:29.942441Z",
     "iopub.status.busy": "2025-02-04T01:19:29.942155Z",
     "iopub.status.idle": "2025-02-04T01:21:59.264893Z",
     "shell.execute_reply": "2025-02-04T01:21:59.264192Z",
     "shell.execute_reply.started": "2025-02-04T01:19:29.942418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 took 6551.30 miliseconds: train loss 2.6143, train bits/byte 0.4715, val loss 2.0554, val bits/byte 0.3707, \n",
      "Epoch 2 took 6601.88 miliseconds: train loss 2.0109, train bits/byte 0.3626, val loss 1.8814, val bits/byte 0.3393, \n",
      "Epoch 3 took 6741.45 miliseconds: train loss 1.8310, train bits/byte 0.3302, val loss 1.7893, val bits/byte 0.3227, \n",
      "Epoch 4 took 6882.44 miliseconds: train loss 1.7267, train bits/byte 0.3114, val loss 1.7121, val bits/byte 0.3088, \n",
      "Epoch 5 took 7045.88 miliseconds: train loss 1.6684, train bits/byte 0.3009, val loss 1.6824, val bits/byte 0.3034, \n",
      "Epoch 6 took 7204.28 miliseconds: train loss 1.6146, train bits/byte 0.2912, val loss 1.6510, val bits/byte 0.2977, \n",
      "Epoch 7 took 7383.98 miliseconds: train loss 1.5806, train bits/byte 0.2850, val loss 1.6355, val bits/byte 0.2949, \n",
      "Epoch 8 took 7533.01 miliseconds: train loss 1.5463, train bits/byte 0.2789, val loss 1.5964, val bits/byte 0.2879, \n",
      "Epoch 9 took 7572.81 miliseconds: train loss 1.5213, train bits/byte 0.2743, val loss 1.5658, val bits/byte 0.2824, \n",
      "Epoch 10 took 7465.72 miliseconds: train loss 1.5028, train bits/byte 0.2710, val loss 1.5616, val bits/byte 0.2816, \n",
      "Epoch 11 took 7300.97 miliseconds: train loss 1.4789, train bits/byte 0.2667, val loss 1.5412, val bits/byte 0.2779, \n",
      "Epoch 12 took 7194.94 miliseconds: train loss 1.4669, train bits/byte 0.2645, val loss 1.5439, val bits/byte 0.2784, \n",
      "Epoch 13 took 7123.96 miliseconds: train loss 1.4480, train bits/byte 0.2611, val loss 1.5356, val bits/byte 0.2769, \n",
      "Epoch 14 took 7091.28 miliseconds: train loss 1.4369, train bits/byte 0.2591, val loss 1.5281, val bits/byte 0.2756, \n",
      "Epoch 15 took 7081.38 miliseconds: train loss 1.4269, train bits/byte 0.2573, val loss 1.5103, val bits/byte 0.2724, \n",
      "Epoch 16 took 7083.34 miliseconds: train loss 1.4076, train bits/byte 0.2538, val loss 1.5172, val bits/byte 0.2736, \n",
      "Epoch 17 took 7143.60 miliseconds: train loss 1.4013, train bits/byte 0.2527, val loss 1.5078, val bits/byte 0.2719, \n",
      "Epoch 18 took 7180.53 miliseconds: train loss 1.3924, train bits/byte 0.2511, val loss 1.5165, val bits/byte 0.2735, \n",
      "Epoch 19 took 7257.67 miliseconds: train loss 1.3843, train bits/byte 0.2496, val loss 1.4871, val bits/byte 0.2682, \n",
      "Epoch 20 took 7315.04 miliseconds: train loss 1.3776, train bits/byte 0.2484, val loss 1.5018, val bits/byte 0.2708, \n",
      "\n",
      "Generated Text:\n",
      "To be or not to be that is the question\n",
      "To the comfort of the man have adversing\n",
      "The presence is heavy shall be reason in the world.\n",
      "\n",
      "COMINIUS:\n",
      "The father comes here in the soul with the blood\n",
      "Of his heart the office of your brother\n",
      "Shall respect and fault from him to strive\n",
      "The dead to be made the king of Thomas sea,\n",
      "And by the very thought of her love is a witness\n",
      "Than his hour is a wife and honour for thy distraction,\n",
      "The that may do not such as the sight.\n",
      "\n",
      "KING RICHARD III:\n",
      "There is the other stroke that is one to see\n",
      "Where is th\n",
      "\n",
      "--------------------------------------------------\n",
      "Training finished!\n",
      "Training time excluding generation: 7315.039157867432 miliseconds\n"
     ]
    }
   ],
   "source": [
    "model = ShakeLSTM(vocab_size=Config.vocab_size).to(Config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "prompt = \"To be or not to be that is the question\"\n",
    "\n",
    "\n",
    "for epoch in range(Config.max_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_bpb = train_epoch(model, optimizer, criterion)\n",
    "    val_loss, val_bpb = evaluate(model, criterion)  \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    \n",
    "    print(f'Epoch {epoch+1} took {dt:.2f} miliseconds: '\n",
    "          f'train loss {train_loss:.4f}, '\n",
    "          f'train bits/byte {train_bpb:.4f}, '\n",
    "          f'val loss {val_loss:.4f}, '\n",
    "          f'val bits/byte {val_bpb:.4f}, ')\n",
    "    \n",
    "generated_text = generate(model, prompt, max_length=500, temperature=0.5)\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\\n\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Training finished!\")\n",
    "print(f'Training time excluding generation: {(t1 - t0) * 1000} miliseconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved very good metrics, with training loss decreasing from 2.61 to 1.38 and validation loss improving from 2.06 to 1.50 over 20 epochs. However, the generated text shows a fundamental limitation of LSTM architectures: their milited ability to maintain long-term dependencies through hidden states. While the model successfully learned to generate descent English words and even some Shakespearean-style phrases (terms like \"COMINIUS\" and \"KING RICHARD III\"), it fails to maintain coherence across sentences. The output do not have logical flow and meaningful context, giving text that is syntactically valid but meaningless. This shows one of the key problems that Transformer architectures addressed through their self-attention mechanism, which can more effectively capture and maintain long-range dependencies in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6496174,
     "sourceId": 10491667,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
