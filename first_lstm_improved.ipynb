{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-05T01:15:24.172776Z",
     "iopub.status.busy": "2025-02-05T01:15:24.172462Z",
     "iopub.status.idle": "2025-02-05T01:15:29.074119Z",
     "shell.execute_reply": "2025-02-05T01:15:29.073395Z",
     "shell.execute_reply.started": "2025-02-05T01:15:24.172752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:15:29.075715Z",
     "iopub.status.busy": "2025-02-05T01:15:29.075306Z",
     "iopub.status.idle": "2025-02-05T01:15:29.325951Z",
     "shell.execute_reply": "2025-02-05T01:15:29.324946Z",
     "shell.execute_reply.started": "2025-02-05T01:15:29.075668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/tinyshake/tiny_shake.txt', \"r\", encoding=\"utf-8\") as my_file:\n",
    "    text = my_file.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:15:29.328163Z",
     "iopub.status.busy": "2025-02-05T01:15:29.327837Z",
     "iopub.status.idle": "2025-02-05T01:15:29.409721Z",
     "shell.execute_reply": "2025-02-05T01:15:29.408609Z",
     "shell.execute_reply.started": "2025-02-05T01:15:29.328139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 64\n",
    "    block_size = 128\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    learning_rate = 1e-3\n",
    "    grad_clip = 1.0\n",
    "    max_epochs = 20\n",
    "    weight_init_range = 0.1\n",
    "    num_batches_per_epoch = 100\n",
    "    num_eval_batches = 10\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "# Code to get the training and validation batches\n",
    "def get_batch(split, block_size=Config.block_size, batch_size=Config.batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch of data for training or testing.\n",
    "    \"\"\"\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for i in ix:\n",
    "        x_batch.append(data[i:i+block_size])\n",
    "        y_batch.append(data[i+1:i+1+block_size])\n",
    "\n",
    "    x = torch.stack(x_batch)\n",
    "    y = torch.stack(y_batch)\n",
    "    x, y = x.to(Config.device), y.to(Config.device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Generate text\n",
    "def generate(model, start_text, max_length, temperature):\n",
    "    model.eval()\n",
    "    chars = torch.tensor(encode(start_text)).unsqueeze(0).to(Config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(chars)\n",
    "            next_char_logits = logits[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_char_logits, dim=0)\n",
    "            next_char = torch.multinomial(probs, 1)\n",
    "            chars = torch.cat([chars, next_char.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return decode(chars[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:15:29.411224Z",
     "iopub.status.busy": "2025-02-05T01:15:29.410846Z",
     "iopub.status.idle": "2025-02-05T01:15:32.557099Z",
     "shell.execute_reply": "2025-02-05T01:15:32.556247Z",
     "shell.execute_reply.started": "2025-02-05T01:15:29.411161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram entropy: 4.7794 bits per character (0.5974 bits per byte)\n",
      "2-gram entropy: 4.1588 bits per character (0.5199 bits per byte)\n",
      "3-gram entropy: 3.6899 bits per character (0.4612 bits per byte)\n",
      "4-gram entropy: 3.3077 bits per character (0.4135 bits per byte)\n",
      "Conditional entropy with context size 1: 3.5383 bits per character (0.4423 bits per byte)\n",
      "Conditional entropy with context size 2: 2.7520 bits per character (0.3440 bits per byte)\n",
      "Conditional entropy with context size 3: 2.1610 bits per character (0.2701 bits per byte)\n",
      "Conditional entropy with context size 4: 1.7672 bits per character (0.2209 bits per byte)\n"
     ]
    }
   ],
   "source": [
    "def calculate_ngram_entropies(text, max_n = 4):\n",
    "    \"\"\"\n",
    "    Calculate entropy for different n-gram sizes up to max_n.\n",
    "    Returns both bits per character and bits per byte.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n in range(1, max_n + 1):\n",
    "        # Create overlapping n-grams\n",
    "        ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        \n",
    "        # Count frequencies\n",
    "        counts = Counter(ngrams)\n",
    "        total = len(ngrams)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        probs = [count/total for count in counts.values()]\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = -sum(p * log2(p) for p in probs)\n",
    "        \n",
    "        # Normalize by n to get per-character entropy\n",
    "        normalized_entropy = entropy / n\n",
    "        # Calculate bits per byte\n",
    "        bits_per_byte = normalized_entropy / 8\n",
    "        \n",
    "        results[n] = (normalized_entropy, bits_per_byte)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def calculate_conditional_entropy(text, context_size):\n",
    "    \"\"\"\n",
    "    Calculate conditional entropy H(X|Y) where Y is the context.\n",
    "    Returns both bits per character and bits per byte.\n",
    "    \"\"\"\n",
    "    # Get all possible contexts and their following characters\n",
    "    contexts = {}\n",
    "    for i in range(len(text) - context_size):\n",
    "        context = text[i:i+context_size]\n",
    "        next_char = text[i+context_size]\n",
    "        if context not in contexts:\n",
    "            contexts[context] = []\n",
    "        contexts[context].append(next_char)\n",
    "    \n",
    "    # Calculate conditional entropy\n",
    "    total_contexts = sum(len(chars) for chars in contexts.values())\n",
    "    entropy = 0\n",
    "    \n",
    "    for context, next_chars in contexts.items():\n",
    "        # Probability of this context\n",
    "        context_prob = len(next_chars) / total_contexts\n",
    "        \n",
    "        # Calculate entropy for characters following this context\n",
    "        char_counts = Counter(next_chars)\n",
    "        char_probs = [count/len(next_chars) for count in char_counts.values()]\n",
    "        context_entropy = -sum(p * log2(p) for p in char_probs)\n",
    "        \n",
    "        entropy += context_prob * context_entropy\n",
    "    \n",
    "    bits_per_byte = entropy / 8\n",
    "    return entropy, bits_per_byte\n",
    "\n",
    "# Calculate n-gram entropies\n",
    "ngram_entropies = calculate_ngram_entropies(text)\n",
    "for n, (bpc, bpb) in ngram_entropies.items():\n",
    "    print(f\"{n}-gram entropy: {bpc:.4f} bits per character ({bpb:.4f} bits per byte)\")\n",
    "\n",
    "# Calculate conditional entropies for different context sizes\n",
    "for context_size in range(1, 5):\n",
    "    bpc, bpb = calculate_conditional_entropy(text, context_size)\n",
    "    print(f\"Conditional entropy with context size {context_size}: \"\n",
    "          f\"{bpc:.4f} bits per character ({bpb:.4f} bits per byte)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve training we are going to use **torch.autocast**. It automatically converts certain operations to run in FP16 precision instead of FP32 where it's safe to do so. This includes matrix multiplications in the LSTM layers, the embedding layer operations, linear layer transformations while It keeps other operations that need higher precision (like loss calculations and certain accumulations) in FP32 for numerical stability. \n",
    "\n",
    "The GradScaler works together with autocast to scale up the loss to prevent underflow in FP16 gradients, unscale gradients before gradient clipping and handle the optimizer step safely with mixed precision. Underflow occurs when a calculation produces a number that is too small to be represented in the available precision format, causing it to be rounded to zero. FP16 can only represent numbers between approximately 6e-5 and 65504 when gradients become very small during backpropagation they might fall below 6e-5, \n",
    "if this happens, they get rounded to 0, effectively stopping any learning in those parameters. Example:\n",
    "\n",
    "* Without scaling: 1e-8 * 0.5 = 0 (underflow in FP16)\n",
    "* With scaling: (1e-8 * 1000) * 0.5 = 5e-6, then divide by 1000 = 5e-9 (preserves the small value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:18:01.126271Z",
     "iopub.status.busy": "2025-02-05T01:18:01.125955Z",
     "iopub.status.idle": "2025-02-05T01:18:01.140304Z",
     "shell.execute_reply": "2025-02-05T01:18:01.139282Z",
     "shell.execute_reply.started": "2025-02-05T01:18:01.126249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a1e0076500b3>:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "class ShakeLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, Config.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(Config.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=Config.embedding_dim,\n",
    "            hidden_size=Config.hidden_dim,\n",
    "            num_layers=Config.num_layers,\n",
    "            dropout=Config.dropout,\n",
    "            batch_first=True # (batch_size, seq_length, input_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.dropout)\n",
    "        self.fc = nn.Linear(Config.hidden_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize embeddings and linear layer\n",
    "        self.embedding.weight.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "        self.fc.weight.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "        # Initialize LSTM weights and biases\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "            elif 'bias' in name:\n",
    "                # Initialize all biases to zero\n",
    "                param.data.zero_()\n",
    "                # Set forget gate bias to 1\n",
    "                if 'bias_ih' in name or 'bias_hh' in name:\n",
    "                    n = param.size(0)\n",
    "                    start, end = n//4, n//2\n",
    "                    param.data[start:end].fill_(1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # 64, 128 --> 64, 128, 256\n",
    "        normalized = self.layer_norm(embedded) \n",
    "        lstm_out, _ = self.lstm(normalized) #64, 128, 512\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        logits = self.fc(dropped) # 64, 128, 65\n",
    "        return logits\n",
    "\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "def train_epoch(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "\n",
    "    for i in range(Config.num_batches_per_epoch):\n",
    "        x, y = get_batch('train')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Enable automatic mixed precision and converts certain operations to run in FP16 where beneficial\n",
    "        # while keeping other operations in FP32 where needed for numerical stability.\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            logits = model(x)\n",
    "            B, T, C = logits.shape\n",
    "            loss = criterion(logits.view(-1, C), y.view(-1))\n",
    "            perplexity = torch.exp(loss)\n",
    "\n",
    "        # Scale to prevent underflow\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)  # Unscale before clipping\n",
    "        clip_grad_norm_(model.parameters(), max_norm=Config.grad_clip)\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_perplexity += perplexity.item()\n",
    "\n",
    "    avg_loss = total_loss / Config.num_batches_per_epoch\n",
    "    bits_per_char = avg_loss / math.log(2)  # Convert nats to bits\n",
    "    bits_per_byte = bits_per_char / 8  # Convert bits per char to bits per byte\n",
    "\n",
    "    return avg_loss, bits_per_byte\n",
    "\n",
    "# Modified evaluate function with memory optimizations\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(Config.num_eval_batches):\n",
    "        x, y = get_batch('val')\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            logits = model(x)\n",
    "            B, T, C = logits.shape\n",
    "            loss = criterion(logits.view(-1, C), y.view(-1))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Explicitly clear cache every few batches\n",
    "        if i % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_loss = total_loss / Config.num_eval_batches\n",
    "    bits_per_char = avg_loss / math.log(2)\n",
    "    bits_per_byte = bits_per_char / 8\n",
    "    \n",
    "    return avg_loss, bits_per_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:18:01.724318Z",
     "iopub.status.busy": "2025-02-05T01:18:01.724004Z",
     "iopub.status.idle": "2025-02-05T01:18:54.861828Z",
     "shell.execute_reply": "2025-02-05T01:18:54.861091Z",
     "shell.execute_reply.started": "2025-02-05T01:18:01.724296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 took 2352.05 ms: train loss 2.5979, train bits/byte 0.4685, val loss 2.0597, val bits/byte 0.3714, tokens/sec: 348291.45\n",
      "Epoch 2 took 2278.77 ms: train loss 2.0094, train bits/byte 0.3624, val loss 1.8767, val bits/byte 0.3384, tokens/sec: 359491.77\n",
      "Epoch 3 took 2298.70 ms: train loss 1.8339, train bits/byte 0.3307, val loss 1.7796, val bits/byte 0.3209, tokens/sec: 356374.63\n",
      "Epoch 4 took 2305.48 ms: train loss 1.7333, train bits/byte 0.3126, val loss 1.7289, val bits/byte 0.3118, tokens/sec: 355327.34\n",
      "Epoch 5 took 2305.48 ms: train loss 1.6712, train bits/byte 0.3014, val loss 1.6955, val bits/byte 0.3058, tokens/sec: 355327.20\n",
      "Epoch 6 took 2312.34 ms: train loss 1.6190, train bits/byte 0.2920, val loss 1.6499, val bits/byte 0.2975, tokens/sec: 354272.65\n",
      "Epoch 7 took 2313.96 ms: train loss 1.5813, train bits/byte 0.2852, val loss 1.6211, val bits/byte 0.2923, tokens/sec: 354025.74\n",
      "Epoch 8 took 2330.04 ms: train loss 1.5517, train bits/byte 0.2798, val loss 1.6015, val bits/byte 0.2888, tokens/sec: 351582.45\n",
      "Epoch 9 took 2333.47 ms: train loss 1.5275, train bits/byte 0.2755, val loss 1.5892, val bits/byte 0.2866, tokens/sec: 351064.56\n",
      "Epoch 10 took 2332.03 ms: train loss 1.5073, train bits/byte 0.2718, val loss 1.5639, val bits/byte 0.2820, tokens/sec: 351282.50\n",
      "Epoch 11 took 2340.70 ms: train loss 1.4877, train bits/byte 0.2683, val loss 1.5395, val bits/byte 0.2776, tokens/sec: 349981.00\n",
      "Epoch 12 took 2349.49 ms: train loss 1.4725, train bits/byte 0.2655, val loss 1.5371, val bits/byte 0.2772, tokens/sec: 348671.50\n",
      "Epoch 13 took 2350.60 ms: train loss 1.4549, train bits/byte 0.2624, val loss 1.5186, val bits/byte 0.2739, tokens/sec: 348507.47\n",
      "Epoch 14 took 2362.69 ms: train loss 1.4405, train bits/byte 0.2598, val loss 1.5568, val bits/byte 0.2808, tokens/sec: 346722.74\n",
      "Epoch 15 took 2358.65 ms: train loss 1.4253, train bits/byte 0.2570, val loss 1.5119, val bits/byte 0.2726, tokens/sec: 347316.80\n",
      "Epoch 16 took 2361.14 ms: train loss 1.4227, train bits/byte 0.2566, val loss 1.5259, val bits/byte 0.2752, tokens/sec: 346950.70\n",
      "Epoch 17 took 2381.39 ms: train loss 1.4110, train bits/byte 0.2545, val loss 1.5177, val bits/byte 0.2737, tokens/sec: 344001.41\n",
      "Epoch 18 took 2381.25 ms: train loss 1.3971, train bits/byte 0.2520, val loss 1.5082, val bits/byte 0.2720, tokens/sec: 344021.56\n",
      "Epoch 19 took 2390.18 ms: train loss 1.3868, train bits/byte 0.2501, val loss 1.5059, val bits/byte 0.2716, tokens/sec: 342735.61\n",
      "Epoch 20 took 2394.50 ms: train loss 1.3793, train bits/byte 0.2487, val loss 1.4710, val bits/byte 0.2653, tokens/sec: 342117.45\n",
      "\n",
      "Generated Text:\n",
      "To be or not to be that is the question to be so.\n",
      "\n",
      "MARIANA:\n",
      "O they think the wife, when I will be not\n",
      "An ear will have as the love with sons and back,\n",
      "And therefore I have a father and you,\n",
      "That I shall do thee to a voice, whose\n",
      "sentence to hear thy heart that he is the princes and medity;\n",
      "And why he has so with thee with the tongue and the\n",
      "defend the battle been some service to heaven.\n",
      "\n",
      "BRUTUS:\n",
      "Now, and the body that I did make a word:\n",
      "For I shall be a most brother of the\n",
      "grace and the patient and shall be your brother.\n",
      "\n",
      "BUCKINGHAM:\n",
      "\n",
      "--------------------------------------------------\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "model = ShakeLSTM(vocab_size=Config.vocab_size).to(Config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "prompt = \"To be or not to be that is the question\"\n",
    "\n",
    "for epoch in range(Config.max_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_bpb = train_epoch(model, optimizer, criterion)\n",
    "    val_loss, val_bpb = evaluate(model, criterion)  \n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0  # time in seconds\n",
    "    \n",
    "    # Calculate tokens processed\n",
    "    tokens_processed = Config.batch_size * Config.block_size * Config.num_batches_per_epoch\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    \n",
    "    print(f'Epoch {epoch+1} took {dt*1000:.2f} ms: '\n",
    "          f'train loss {train_loss:.4f}, '\n",
    "          f'train bits/byte {train_bpb:.4f}, '\n",
    "          f'val loss {val_loss:.4f}, '\n",
    "          f'val bits/byte {val_bpb:.4f}, '\n",
    "          f'tokens/sec: {tokens_per_sec:.2f}')\n",
    "    \n",
    "generated_text = generate(model, prompt, max_length=500, temperature=0.5)\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\\n\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this technique we were able to reduce the training time, in each epoch, from 7315.04 ms to 2394.50, in order words, a 67% improvement, not bad for a 7 years old GPU. However, if we use a modern GPU, and im not even talking about A100, I'm talking about RTX4090, we would smash these numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6598495,
     "sourceId": 10655744,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
