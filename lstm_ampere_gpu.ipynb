{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958962c4",
   "metadata": {},
   "source": [
    "## A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "with open('./tiny_shake.txt', \"r\", encoding=\"utf-8\") as my_file:\n",
    "    text = my_file.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "class Config:\n",
    "    batch_size = 64\n",
    "    block_size = 128\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    learning_rate = 1e-3\n",
    "    grad_clip = 1.0\n",
    "    max_epochs = 20\n",
    "    weight_init_range = 0.1\n",
    "    num_batches_per_epoch = 100\n",
    "    num_eval_batches = 10\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "# Code to get the training and validation batches\n",
    "def get_batch(split, block_size=Config.block_size, batch_size=Config.batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch of data for training or testing.\n",
    "    \"\"\"\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    for i in ix:\n",
    "        x_batch.append(data[i:i+block_size])\n",
    "        y_batch.append(data[i+1:i+1+block_size])\n",
    "\n",
    "    x = torch.stack(x_batch)\n",
    "    y = torch.stack(y_batch)\n",
    "    x, y = x.to(Config.device), y.to(Config.device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Generate text\n",
    "def generate(model, start_text, max_length, temperature):\n",
    "    model.eval()\n",
    "    chars = torch.tensor(encode(start_text)).unsqueeze(0).to(Config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(chars)\n",
    "            next_char_logits = logits[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_char_logits, dim=0)\n",
    "            next_char = torch.multinomial(probs, 1)\n",
    "            chars = torch.cat([chars, next_char.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return decode(chars[0].tolist())\n",
    "\n",
    "\n",
    "class ShakeLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, Config.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(Config.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=Config.embedding_dim,\n",
    "            hidden_size=Config.hidden_dim,\n",
    "            num_layers=Config.num_layers,\n",
    "            dropout=Config.dropout,\n",
    "            batch_first=True # (batch_size, seq_length, input_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.dropout)\n",
    "        self.fc = nn.Linear(Config.hidden_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize embeddings and linear layer\n",
    "        self.embedding.weight.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "        self.fc.weight.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "        # Initialize LSTM weights and biases\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param.data.uniform_(-Config.weight_init_range, Config.weight_init_range)\n",
    "            elif 'bias' in name:\n",
    "                # Initialize all biases to zero\n",
    "                param.data.zero_()\n",
    "                # Set forget gate bias to 1\n",
    "                if 'bias_ih' in name or 'bias_hh' in name:\n",
    "                    n = param.size(0)\n",
    "                    start, end = n//4, n//2\n",
    "                    param.data[start:end].fill_(1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # 64, 128 --> 64, 128, 256\n",
    "        normalized = self.layer_norm(embedded) \n",
    "        lstm_out, _ = self.lstm(normalized) #64, 128, 512\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        logits = self.fc(dropped) # 64, 128, 65\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    \n",
    "    for i in range(Config.num_batches_per_epoch):\n",
    "        x, y = get_batch('train')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(-1, C), y.view(-1))\n",
    "        perplexity = torch.exp(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=Config.grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_perplexity += perplexity.item()\n",
    "        \n",
    "    avg_loss = total_loss / Config.num_batches_per_epoch\n",
    "    # avg_perplexity = total_perplexity / Config.num_batches_per_epoch\n",
    "    bits_per_char = avg_loss / math.log(2)  # Convert nats to bits\n",
    "    bits_per_byte = bits_per_char / 8  # Convert bits per char to bits per byte\n",
    "    \n",
    "    return avg_loss, bits_per_byte\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    \n",
    "    for i in range(Config.num_eval_batches):\n",
    "        x, y = get_batch('val')\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = criterion(logits.view(-1, C), y.view(-1))\n",
    "        perplexity = torch.exp(loss)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_perplexity += perplexity.item()\n",
    "        \n",
    "    avg_loss = total_loss / Config.num_eval_batches\n",
    "    # avg_perplexity = total_perplexity / Config.num_eval_batches\n",
    "    bits_per_char = avg_loss / math.log(2)  # Convert nats to bits\n",
    "    bits_per_byte = bits_per_char / 8  # Convert bits per char to bits per byte\n",
    "    \n",
    "    return avg_loss, bits_per_byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52488d84-853e-41c7-b0a2-7bee433728fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 took 1885.98 ms: train loss 2.6069, train bits/byte 0.4701, val loss 2.0776, val bits/byte 0.3747, tokens/sec: 434364.16\n",
      "Epoch 2 took 1274.57 ms: train loss 2.0076, train bits/byte 0.3620, val loss 1.8829, val bits/byte 0.3395, tokens/sec: 642724.89\n",
      "Epoch 3 took 1139.59 ms: train loss 1.8373, train bits/byte 0.3313, val loss 1.7750, val bits/byte 0.3201, tokens/sec: 718856.97\n",
      "Epoch 4 took 1143.90 ms: train loss 1.7366, train bits/byte 0.3132, val loss 1.7239, val bits/byte 0.3109, tokens/sec: 716147.76\n",
      "Epoch 5 took 1223.54 ms: train loss 1.6714, train bits/byte 0.3014, val loss 1.6660, val bits/byte 0.3004, tokens/sec: 669533.67\n",
      "Epoch 6 took 1557.08 ms: train loss 1.6248, train bits/byte 0.2930, val loss 1.6312, val bits/byte 0.2942, tokens/sec: 526111.76\n",
      "Epoch 7 took 1154.43 ms: train loss 1.5852, train bits/byte 0.2859, val loss 1.6264, val bits/byte 0.2933, tokens/sec: 709612.27\n",
      "Epoch 8 took 1118.24 ms: train loss 1.5539, train bits/byte 0.2802, val loss 1.6035, val bits/byte 0.2892, tokens/sec: 732581.25\n",
      "Epoch 9 took 1113.41 ms: train loss 1.5278, train bits/byte 0.2755, val loss 1.5816, val bits/byte 0.2852, tokens/sec: 735756.77\n",
      "Epoch 10 took 1111.65 ms: train loss 1.5083, train bits/byte 0.2720, val loss 1.5660, val bits/byte 0.2824, tokens/sec: 736924.34\n",
      "Epoch 11 took 1110.86 ms: train loss 1.4901, train bits/byte 0.2687, val loss 1.5607, val bits/byte 0.2814, tokens/sec: 737445.96\n",
      "Epoch 12 took 1118.54 ms: train loss 1.4719, train bits/byte 0.2654, val loss 1.5347, val bits/byte 0.2768, tokens/sec: 732380.91\n",
      "Epoch 13 took 1129.35 ms: train loss 1.4582, train bits/byte 0.2630, val loss 1.5202, val bits/byte 0.2741, tokens/sec: 725375.03\n",
      "Epoch 14 took 1301.35 ms: train loss 1.4466, train bits/byte 0.2609, val loss 1.5264, val bits/byte 0.2753, tokens/sec: 629499.85\n",
      "Epoch 15 took 1172.09 ms: train loss 1.4341, train bits/byte 0.2586, val loss 1.5268, val bits/byte 0.2753, tokens/sec: 698925.11\n",
      "Epoch 16 took 1150.48 ms: train loss 1.4182, train bits/byte 0.2558, val loss 1.5053, val bits/byte 0.2715, tokens/sec: 712051.78\n",
      "Epoch 17 took 1123.61 ms: train loss 1.4124, train bits/byte 0.2547, val loss 1.4932, val bits/byte 0.2693, tokens/sec: 729078.12\n",
      "Epoch 18 took 1113.86 ms: train loss 1.3977, train bits/byte 0.2521, val loss 1.5070, val bits/byte 0.2718, tokens/sec: 735461.80\n",
      "Epoch 19 took 1112.94 ms: train loss 1.3965, train bits/byte 0.2518, val loss 1.4999, val bits/byte 0.2705, tokens/sec: 736069.17\n",
      "Epoch 20 took 1255.09 ms: train loss 1.3876, train bits/byte 0.2502, val loss 1.5016, val bits/byte 0.2708, tokens/sec: 652700.09\n",
      "\n",
      "Generated Text:\n",
      "To be or not to be that is the question, for the grief\n",
      "That have dissented so and promise them well.\n",
      "Shall I shall not be both of my father's changes.\n",
      "\n",
      "First Servingman:\n",
      "And here's your proverse, stay the use of the soldiers,\n",
      "And like the noble country's enemies of the world,\n",
      "That marry a charge and despite of the king endure\n",
      "In his part for princes of a hand, and never\n",
      "I can then have unconscience of your time\n",
      "And he are stand of the people in the sentence.\n",
      "\n",
      "KING RICHARD III:\n",
      "What's your power,\n",
      "The world he may not she shall be the \n",
      "\n",
      "--------------------------------------------------\n",
      "Training finished!\n",
      "Training time excluding generation: 1255.094051361084 miliseconds\n"
     ]
    }
   ],
   "source": [
    "#torch.set_float32_matmul_precision(\"high\")\n",
    "model = ShakeLSTM(vocab_size=Config.vocab_size).to(Config.device)\n",
    "model = torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "prompt = \"To be or not to be that is the question\"\n",
    "\n",
    "for epoch in range(Config.max_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_bpb = train_epoch(model, optimizer, criterion)\n",
    "    val_loss, val_bpb = evaluate(model, criterion)  \n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    tokens_processed = Config.batch_size * Config.block_size * Config.num_batches_per_epoch\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    \n",
    "    print(f'Epoch {epoch+1} took {dt*1000:.2f} ms: '\n",
    "          f'train loss {train_loss:.4f}, '\n",
    "          f'train bits/byte {train_bpb:.4f}, '\n",
    "          f'val loss {val_loss:.4f}, '\n",
    "          f'val bits/byte {val_bpb:.4f}, '\n",
    "          f'tokens/sec: {tokens_per_sec:.2f}')\n",
    "    \n",
    "generated_text = generate(model, prompt, max_length=500, temperature=0.5)\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\\n\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Training finished!\")\n",
    "print(f'Training time excluding generation: {(t1 - t0) * 1000} miliseconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a39153",
   "metadata": {},
   "source": [
    "The first time this notebook was ran, we got an average training time was around 1500 miliseconds, but by just using torch.compile we were able to reduce the training time by 20%. \n",
    "\n",
    "* By just using a A100 GPU, the training was around 50% faster than the optimized code for a T4, we cannot compare the performance. \n",
    "* We have another optimizations availiable for A100 but because our network is small the computational overhead would cause more harm than benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924de1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
